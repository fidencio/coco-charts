name: E2E Tests

on:
  pull_request:
    types:
      - edited
      - opened
      - reopened
      - synchronize
  workflow_dispatch:
  schedule:
    # Run nightly at 00:00 UTC
    - cron: '0 0 * * *'

concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: ${{ github.event_name != 'schedule' }}

permissions:
  contents: read
  issues: write
  pull-requests: read

jobs:
  check-changes:
    name: Check What Changed
    runs-on: ubuntu-24.04
    outputs:
      kata-deploy-version-changed: ${{ steps.check-appversion.outputs.changed }}
      templates-changed: ${{ steps.check-templates.outputs.changed }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Check if kata-deploy dependency version changed
        id: check-appversion
        run: |
          echo "üîç Checking if production kata-deploy dependency version in Chart.yaml changed..."
          
          # For workflow_dispatch and schedule, always run (set to true)
          if [ "${{ github.event_name }}" = "workflow_dispatch" ] || [ "${{ github.event_name }}" = "schedule" ]; then
            echo "changed=true" >> $GITHUB_OUTPUT
            if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
              echo "‚ÑπÔ∏è  Manual trigger - will run all tests"
            else
              echo "‚ÑπÔ∏è  Scheduled nightly run - will run all tests"
            fi
            exit 0
          fi
          
          # Get the base branch for PR
          BASE_SHA="${{ github.event.pull_request.base.sha }}"
          
          # Check if the production kata-deploy version changed (kata-as-coco-runtime)
          # Specifically look for the kata-deploy dependency WITHOUT the "-for-ci" alias
          # We extract the section between "name: kata-deploy" and the next dependency or end
          # and check if the version line changed, excluding the CI variant (0.0.0-dev)
          
          if git diff "${BASE_SHA}" HEAD -- Chart.yaml | \
             grep -B1 -A3 'alias: kata-as-coco-runtime$' | \
             grep -E '^\+.*version:' | \
             grep -v '0.0.0-dev' || \
             git diff "${BASE_SHA}" HEAD -- Chart.yaml | \
             grep -B1 -A3 'alias: kata-as-coco-runtime$' | \
             grep -E '^\-.*version:' | \
             grep -v '0.0.0-dev'; then
            echo "changed=true" >> $GITHUB_OUTPUT
            echo "‚úÖ Production kata-deploy version changed - will run standard deployment test"
          else
            echo "changed=false" >> $GITHUB_OUTPUT
            echo "‚ÑπÔ∏è  Production kata-deploy version unchanged - will skip standard deployment test"
          fi

      - name: Check if templates changed
        id: check-templates
        run: |
          echo "üîç Checking if templates/values changed..."
          
          # For workflow_dispatch and schedule, always run (set to true)
          if [ "${{ github.event_name }}" = "workflow_dispatch" ] || [ "${{ github.event_name }}" = "schedule" ]; then
            echo "changed=true" >> $GITHUB_OUTPUT
            if [ "${{ github.event_name }}" = "workflow_dispatch" ]; then
              echo "‚ÑπÔ∏è  Manual trigger - will run all tests"
            else
              echo "‚ÑπÔ∏è  Scheduled nightly run - will run all tests"
            fi
            exit 0
          fi
          
          BASE_SHA="${{ github.event.pull_request.base.sha }}"
          
          if git diff --name-only "${BASE_SHA}" HEAD | grep -E '(templates/|values\.yaml|values/|Chart\.yaml)'; then
            echo "changed=true" >> $GITHUB_OUTPUT
            echo "‚úÖ Templates/values changed - will run all E2E tests"
          else
            echo "changed=false" >> $GITHUB_OUTPUT
            echo "‚ÑπÔ∏è  No template/values changes - will skip template-dependent tests"
          fi

  e2e-tests:
    name: E2E (${{ matrix.environment.deployment-type }} / ${{ matrix.environment.k8s-distro }}${{ matrix.environment.image-pull-mode && format(' / {0}', matrix.environment.image-pull-mode) || '' }}${{ matrix.environment.test-containerd-upgrade && ' / containerd-upgrade' || '' }})
    runs-on: ubuntu-24.04
    needs: check-changes
    timeout-minutes: 45
    strategy:
      fail-fast: false
      matrix:
        environment: [
          { deployment-type: ci,       k8s-distro: k3s,                       image-pull-mode: nydus,                         test-containerd-upgrade: false },
          { deployment-type: ci,       k8s-distro: k3s,                       image-pull-mode: experimental-force-guest-pull, test-containerd-upgrade: false },
          { deployment-type: ci,       k8s-distro: k0s,                       image-pull-mode: nydus,                         test-containerd-upgrade: false },
          { deployment-type: ci,       k8s-distro: k0s,                       image-pull-mode: experimental-force-guest-pull, test-containerd-upgrade: false },
          { deployment-type: ci,       k8s-distro: rke2,                      image-pull-mode: nydus,                         test-containerd-upgrade: false },
          { deployment-type: ci,       k8s-distro: rke2,                      image-pull-mode: experimental-force-guest-pull, test-containerd-upgrade: false },
          { deployment-type: ci,       k8s-distro: microk8s,                  image-pull-mode: nydus,                         test-containerd-upgrade: false },
          { deployment-type: ci,       k8s-distro: microk8s,                  image-pull-mode: experimental-force-guest-pull, test-containerd-upgrade: false },
          { deployment-type: ci,       k8s-distro: kubeadm-containerd-latest, image-pull-mode: nydus,                         test-containerd-upgrade: false },
          { deployment-type: ci,       k8s-distro: kubeadm-containerd-latest, image-pull-mode: experimental-force-guest-pull, test-containerd-upgrade: false },
          { deployment-type: ci,       k8s-distro: kubeadm-containerd-1.7,    image-pull-mode: nydus,                         test-containerd-upgrade: false },
          { deployment-type: ci,       k8s-distro: kubeadm-containerd-1.7,    image-pull-mode: experimental-force-guest-pull, test-containerd-upgrade: false },
          { deployment-type: ci,       k8s-distro: kubeadm-containerd-1.7,    image-pull-mode: nydus,                         test-containerd-upgrade: true  },
          { deployment-type: ci,       k8s-distro: kubeadm-containerd-1.7,    image-pull-mode: experimental-force-guest-pull, test-containerd-upgrade: true  },
          { deployment-type: standard, k8s-distro: k3s,                       image-pull-mode: nydus,                         test-containerd-upgrade: false },
          { deployment-type: standard, k8s-distro: k3s,                       image-pull-mode: experimental-force-guest-pull, test-containerd-upgrade: false },
          { deployment-type: standard, k8s-distro: k0s,                       image-pull-mode: nydus,                         test-containerd-upgrade: false },
          { deployment-type: standard, k8s-distro: k0s,                       image-pull-mode: experimental-force-guest-pull, test-containerd-upgrade: false },
          { deployment-type: standard, k8s-distro: rke2,                      image-pull-mode: nydus,                         test-containerd-upgrade: false },
          { deployment-type: standard, k8s-distro: rke2,                      image-pull-mode: experimental-force-guest-pull, test-containerd-upgrade: false },
          { deployment-type: standard, k8s-distro: microk8s,                  image-pull-mode: nydus,                         test-containerd-upgrade: false },
          { deployment-type: standard, k8s-distro: microk8s,                  image-pull-mode: experimental-force-guest-pull, test-containerd-upgrade: false },
          { deployment-type: standard, k8s-distro: kubeadm-containerd-latest, image-pull-mode: nydus,                         test-containerd-upgrade: false },
          { deployment-type: standard, k8s-distro: kubeadm-containerd-latest, image-pull-mode: experimental-force-guest-pull, test-containerd-upgrade: false },
          { deployment-type: standard, k8s-distro: kubeadm-containerd-1.7,    image-pull-mode: nydus,                         test-containerd-upgrade: false },
          { deployment-type: standard, k8s-distro: kubeadm-containerd-1.7,    image-pull-mode: experimental-force-guest-pull, test-containerd-upgrade: false },
          { deployment-type: standard, k8s-distro: kubeadm-containerd-1.7,    image-pull-mode: nydus,                         test-containerd-upgrade: true  },
          { deployment-type: standard, k8s-distro: kubeadm-containerd-1.7,    image-pull-mode: experimental-force-guest-pull, test-containerd-upgrade: true  },
        ]
    
    env:
      GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
    
    steps:
      - name: Check if this matrix entry should run
        id: should-run
        run: |
          # CI tests should ALWAYS run
          # Standard tests run only when kata-deploy version changed
          if [ "${{ matrix.environment.deployment-type }}" = "ci" ]; then
            echo "should-run=true" >> $GITHUB_OUTPUT
            echo "‚úÖ CI test will run"
          elif [ "${{ matrix.environment.deployment-type }}" = "standard" ] && [ "${{ needs.check-changes.outputs.kata-deploy-version-changed }}" = "true" ]; then
            echo "should-run=true" >> $GITHUB_OUTPUT
            echo "‚úÖ Standard test will run (kata-deploy version changed)"
          else
            echo "should-run=false" >> $GITHUB_OUTPUT
            echo "‚è≠Ô∏è  Skipping this test (conditions not met)"
          fi
      
      - name: Checkout code
        if: steps.should-run.outputs.should-run == 'true'
        uses: actions/checkout@v4

      - name: Setup Kubernetes cluster (k3s)
        if: steps.should-run.outputs.should-run == 'true' && matrix.environment.k8s-distro == 'k3s'
        uses: ./.github/actions/setup-k8s
        with:
          distribution: k3s
      
      - name: Setup Kubernetes cluster (k0s)
        if: steps.should-run.outputs.should-run == 'true' && matrix.environment.k8s-distro == 'k0s'
        uses: ./.github/actions/setup-k8s
        with:
          distribution: k0s
      
      - name: Setup Kubernetes cluster (rke2)
        if: steps.should-run.outputs.should-run == 'true' && matrix.environment.k8s-distro == 'rke2'
        uses: ./.github/actions/setup-k8s
        with:
          distribution: rke2
      
      - name: Setup Kubernetes cluster (microk8s)
        if: steps.should-run.outputs.should-run == 'true' && matrix.environment.k8s-distro == 'microk8s'
        uses: ./.github/actions/setup-k8s
        with:
          distribution: microk8s
      
      - name: Setup Kubernetes cluster (kubeadm with containerd latest)
        if: steps.should-run.outputs.should-run == 'true' && matrix.environment.k8s-distro == 'kubeadm-containerd-latest'
        uses: ./.github/actions/setup-k8s
        with:
          distribution: kubeadm
          container-runtime: containerd
          runtime-version: latest
      
      - name: Setup Kubernetes cluster (kubeadm with containerd 1.7)
        if: steps.should-run.outputs.should-run == 'true' && matrix.environment.k8s-distro == 'kubeadm-containerd-1.7'
        uses: ./.github/actions/setup-k8s
        with:
          distribution: kubeadm
          container-runtime: containerd
          runtime-version: "1.7"
      
      - name: Setup Kubernetes cluster (kubeadm with CRI-O)
        if: steps.should-run.outputs.should-run == 'true' && matrix.environment.k8s-distro == 'kubeadm-crio'
        uses: ./.github/actions/setup-k8s
        with:
          distribution: kubeadm
          container-runtime: crio

      - name: Determine k8s distribution name
        if: steps.should-run.outputs.should-run == 'true'
        id: k8s-distro-name
        run: |
          case "${{ matrix.environment.k8s-distro }}" in
            k3s) echo "name=k3s" >> $GITHUB_OUTPUT ;;
            k0s) echo "name=k0s" >> $GITHUB_OUTPUT ;;
            rke2) echo "name=rke2" >> $GITHUB_OUTPUT ;;
            microk8s) echo "name=microk8s" >> $GITHUB_OUTPUT ;;
            kubeadm-containerd-latest) echo "name=k8s" >> $GITHUB_OUTPUT ;;
            kubeadm-containerd-1.7) echo "name=k8s" >> $GITHUB_OUTPUT ;;
            kubeadm-crio) echo "name=k8s" >> $GITHUB_OUTPUT ;;
            *) echo "name=k8s" >> $GITHUB_OUTPUT ;;
          esac

      - name: Setup Helm
        if: steps.should-run.outputs.should-run == 'true'
        uses: azure/setup-helm@v4
        with:
          version: '3.13.1'

      - name: Install yq (for CI tests)
        if: steps.should-run.outputs.should-run == 'true' && matrix.environment.deployment-type == 'ci'
        run: |
          echo "üì¶ Installing yq for chart version override..."
          sudo wget -qO /usr/local/bin/yq https://github.com/mikefarah/yq/releases/latest/download/yq_linux_amd64
          sudo chmod +x /usr/local/bin/yq
          yq --version

      - name: Update Helm dependencies
        if: steps.should-run.outputs.should-run == 'true'
        run: |
          echo "üì¶ Updating Helm chart dependencies..."
          if [ "${{ matrix.environment.deployment-type }}" = "ci" ]; then
            echo "üîß CI test detected - using dev chart version from main (0.0.0-dev)..."
            # For CI tests, use the dev chart version published from main
            yq eval -i '.dependencies[0].version = "0.0.0-dev"' Chart.yaml
            echo "‚úÖ Updated Chart.yaml to use dev version 0.0.0-dev"
          else
            echo "üìå Standard test - using released version from Chart.yaml"
          fi
          helm dependency update
          echo "‚úÖ Dependencies updated"

      - name: Get original containerd version
        if: steps.should-run.outputs.should-run == 'true' && matrix.environment.test-containerd-upgrade
        id: original-containerd-version
        run: |
          echo "üîç Checking original containerd version..."
          ORIGINAL_VERSION=$(containerd --version | awk '{print $3}')
          echo "Original containerd version: ${ORIGINAL_VERSION}"
          echo "version=${ORIGINAL_VERSION}" >> $GITHUB_OUTPUT

      - name: Prepare Helm extra args
        if: steps.should-run.outputs.should-run == 'true'
        id: helm-args
        run: |
          # Use CI profile for CI deployment type
          ARGS=""
          if [ "${{ matrix.environment.deployment-type }}" = "ci" ]; then
            ARGS="-f values/profile-ci.yaml"
          fi

          # Add k8s distribution
          ARGS="${ARGS} --set kata-as-coco-runtime.k8sDistribution=${{ steps.k8s-distro-name.outputs.name }}"

          if [ "${{ matrix.environment.image-pull-mode }}" = "experimental-force-guest-pull" ]; then
            # Disable snapshotter setup (set to null) and configure qemu-coco-dev for guest pull
            ARGS="${ARGS} --set kata-as-coco-runtime.snapshotter.setup=null --set kata-as-coco-runtime.shims.qemu-coco-dev.containerd.snapshotter='' --set kata-as-coco-runtime.shims.qemu-coco-dev.containerd.forceGuestPull=true"
          fi
          
          # Add containerd upgrade flags (only for containerd upgrade test)
          if [ "${{ matrix.environment.test-containerd-upgrade }}" == "true" ]; then
            ARGS="${ARGS} --set customContainerd.enabled=true --set customContainerd.tarballUrl=https://github.com/containerd/containerd/releases/download/v2.0.0/containerd-2.0.0-linux-amd64.tar.gz"
          fi
          
          echo "args=${ARGS}" >> $GITHUB_OUTPUT

      - name: Determine deployment parameters
        if: steps.should-run.outputs.should-run == 'true'
        id: deployment-params
        run: |
          if [ "${{ matrix.environment.deployment-type }}" = "ci" ]; then
            echo "release-name=coco-ci" >> $GITHUB_OUTPUT
          else
            echo "release-name=coco" >> $GITHUB_OUTPUT
          fi
          # DaemonSet label is always the same
          echo "daemonset-label=name=kata-as-coco-runtime" >> $GITHUB_OUTPUT
          
          # Detect architecture and select appropriate values file
          ARCH=$(uname -m)
          case "$ARCH" in
            x86_64)
              VALUES_FILE="values.yaml"
              RUNTIME_CLASS_SHIM="qemu-coco-dev"
              ;;
            s390x)
              VALUES_FILE="values/kata-s390x.yaml"
              RUNTIME_CLASS_SHIM="qemu-coco-dev"  # s390x default
              ;;
            *)
              echo "‚ùå Unsupported architecture: $ARCH"
              exit 1
              ;;
          esac
          
          echo "‚ÑπÔ∏è  Detected architecture: ${ARCH}"
          echo "‚ÑπÔ∏è  Using values file: ${VALUES_FILE}"
          
          # Validate values file exists
          if [ ! -f "${VALUES_FILE}" ]; then
            echo "‚ùå Values file not found: ${VALUES_FILE}"
            exit 1
          fi
          
          echo "values-file=${VALUES_FILE}" >> $GITHUB_OUTPUT

          # Get all available shims from values.yaml using new structured format
          # Extract shim names where enabled=true from kata-as-coco-runtime.shims
          # yq outputs each shim on a separate line, so we collect them into a space-separated string
          ALL_SHIMS=$(yq eval '.kata-as-coco-runtime.shims | to_entries | map(select(.value.enabled == true)) | map(.key) | .[]' "${VALUES_FILE}" 2>/dev/null | tr '\n' ' ' | sed 's/ $//' || echo "")
          if [ -z "$ALL_SHIMS" ]; then
            echo "‚ùå Failed to extract shims from ${VALUES_FILE} under kata-as-coco-runtime.shims"
            echo "   Attempting fallback method..."
            # Fallback: try to get all keys (regardless of enabled status)
            ALL_SHIMS=$(yq eval '.kata-as-coco-runtime.shims | keys | .[]' "${VALUES_FILE}" 2>/dev/null | tr '\n' ' ' | sed 's/ $//' || echo "")
            if [ -z "$ALL_SHIMS" ]; then
              echo "‚ùå Fallback also failed - cannot extract shims"
              exit 1
            fi
          fi
          
          echo "‚ÑπÔ∏è  Available shims in ${VALUES_FILE}: ${ALL_SHIMS}"
          
          # Validate that selected shim exists in the values file
          if ! echo "$ALL_SHIMS" | grep -qw "$RUNTIME_CLASS_SHIM"; then
            echo "‚ùå Selected shim '$RUNTIME_CLASS_SHIM' not found in available shims: $ALL_SHIMS"
            exit 1
          fi
          
          # Convert shim to RuntimeClass format
          TEST_RUNTIME_CLASS="kata-${RUNTIME_CLASS_SHIM}"
          
          # Extract ALL shims and convert to RuntimeClass format for verification
          # This ensures we verify that all available runtime classes are properly created
          RUNTIME_CLASSES=""
          for shim in $ALL_SHIMS; do
            runtime_class="kata-${shim}"
            if [ -z "$RUNTIME_CLASSES" ]; then
              RUNTIME_CLASSES="$runtime_class"
            else
              RUNTIME_CLASSES="${RUNTIME_CLASSES} ${runtime_class}"
            fi
          done
          
          echo "expected-runtimeclasses=${RUNTIME_CLASSES}" >> $GITHUB_OUTPUT
          echo "test-pod-name=kata-test-pod" >> $GITHUB_OUTPUT
          echo "first-runtime-class=${TEST_RUNTIME_CLASS}" >> $GITHUB_OUTPUT
          
          echo "‚úÖ Using values file: ${VALUES_FILE}"
          echo "‚úÖ Architecture: ${ARCH}"
          echo "‚úÖ Test pod runtime class: ${TEST_RUNTIME_CLASS} (selected for this hardware)"
          echo "‚úÖ Verify all runtime classes are created: ${RUNTIME_CLASSES}"

      - name: Install chart
        if: steps.should-run.outputs.should-run == 'true'
        uses: ./.github/actions/install-chart
        with:
          release-name: ${{ steps.deployment-params.outputs.release-name }}
          namespace: coco-system
          values-file: ${{ steps.deployment-params.outputs.values-file }}
          extra-args: ${{ steps.helm-args.outputs.args }}
          wait-timeout: 15m

      - name: Verify deployment
        if: steps.should-run.outputs.should-run == 'true'
        uses: ./.github/actions/verify-deployment
        with:
          namespace: coco-system
          expected-runtime-classes: ${{ steps.deployment-params.outputs.expected-runtimeclasses }}
          daemonset-timeout: 15m
          daemonset-label: ${{ steps.deployment-params.outputs.daemonset-label }}

      - name: Get new containerd version
        if: steps.should-run.outputs.should-run == 'true' && matrix.environment.test-containerd-upgrade
        id: new-containerd-version
        run: |
          echo "üîç Checking new containerd version..."
          
          # Retry getting containerd version (might not be immediately available after restart)
          TIMEOUT=30
          ELAPSED=0
          NEW_VERSION=""
          while [ $ELAPSED -lt $TIMEOUT ]; do
            NEW_VERSION=$(containerd --version 2>/dev/null | awk '{print $3}' || echo "")
            if [ -n "$NEW_VERSION" ]; then
              echo "‚úÖ Got containerd version after ${ELAPSED}s"
              break
            fi
            sleep 3
            ELAPSED=$((ELAPSED + 3))
          done
          
          if [ -z "$NEW_VERSION" ]; then
            echo "‚ùå Failed to get containerd version after ${TIMEOUT}s"
            exit 1
          fi
          
          echo "New containerd version: ${NEW_VERSION}"
          echo "version=${NEW_VERSION}" >> $GITHUB_OUTPUT

      - name: Compare containerd versions
        if: steps.should-run.outputs.should-run == 'true' && matrix.environment.test-containerd-upgrade
        run: |
          echo "üìä Comparing containerd versions..."
          echo ""
          echo "Original version: ${{ steps.original-containerd-version.outputs.version }}"
          echo "New version:      ${{ steps.new-containerd-version.outputs.version }}"
          echo ""
          
          if [ "${{ steps.original-containerd-version.outputs.version }}" = "${{ steps.new-containerd-version.outputs.version }}" ]; then
            echo "‚ùå Containerd version did not change!"
            echo "Expected upgrade from ${{ steps.original-containerd-version.outputs.version }} to a different version"
            exit 1
          fi
          
          echo "‚úÖ Containerd version successfully upgraded!"
          echo "   From: ${{ steps.original-containerd-version.outputs.version }}"
          echo "   To:   ${{ steps.new-containerd-version.outputs.version }}"

      - name: Run test pod
        if: steps.should-run.outputs.should-run == 'true'
        uses: ./.github/actions/run-test-pod
        with:
          runtime-class: ${{ steps.deployment-params.outputs.first-runtime-class }}
          namespace: default
          pod-name: ${{ steps.deployment-params.outputs.test-pod-name }}
          timeout: 5m

      - name: Collect logs on failure
        if: failure()
        run: |
          echo "üìã Collecting diagnostic information..."
          
          echo "=== Helm releases ==="
          helm list -A
          
          echo ""
          echo "=== All pods ==="
          kubectl get pods -A
          
          echo ""
          echo "=== DaemonSet status ==="
          kubectl get daemonset -n coco-system
          
          echo ""
          echo "=== RuntimeClasses ==="
          kubectl get runtimeclass
          
          echo ""
          echo "=== Kata Containers Configuration ==="
          RUNTIME_CLASS="${{ steps.deployment-params.outputs.first-runtime-class }}"
          # Extract the configuration file name from RuntimeClass (e.g., kata-qemu-coco-dev -> qemu-coco-dev)
          CONFIG_NAME="${RUNTIME_CLASS#kata-}"
          
          # Get installation prefix from Helm values (defaults)
          INSTALL_PREFIX="/opt/kata"
          
          CONFIG_FILE="${INSTALL_PREFIX}/share/defaults/kata-containers/configuration-${CONFIG_NAME}.toml"
          
          echo "RuntimeClass: ${RUNTIME_CLASS}"
          echo "Configuration file: ${CONFIG_FILE}"
          echo ""
          
          # Get node name
          NODE=$(kubectl get nodes -o jsonpath='{.items[0].metadata.name}')
          echo "Reading from node: ${NODE}"
          echo ""
          
          # Create a temporary pod to read the configuration file
          POD_NAME="kata-config-reader-$(date +%s)"
          cat <<EOF | kubectl apply -f - >/dev/null
          apiVersion: v1
          kind: Pod
          metadata:
            name: ${POD_NAME}
            namespace: default
          spec:
            hostPID: true
            hostNetwork: true
            nodeName: ${NODE}
            containers:
            - name: reader
              image: busybox:latest
              command: ['sleep', '60']
              volumeMounts:
              - name: host-root
                mountPath: /host
                readOnly: true
              securityContext:
                privileged: true
            volumes:
            - name: host-root
              hostPath:
                path: /
            restartPolicy: Never
          EOF
          
          # Wait for pod to be ready
          echo "Waiting for reader pod..."
          kubectl wait --for=condition=Ready pod/${POD_NAME} -n default --timeout=30s >/dev/null 2>&1 || true
          sleep 2
          
          # Read the configuration file
          echo "--- ${CONFIG_FILE} ---"
          if kubectl exec ${POD_NAME} -n default -- cat /host${CONFIG_FILE} 2>/dev/null; then
            echo ""
            echo "‚úÖ Configuration file displayed successfully"
          else
            echo ""
            echo "‚ùå Failed to read configuration file"
            echo ""
            echo "Attempting to list available configurations:"
            kubectl exec ${POD_NAME} -n default -- ls -la /host${INSTALL_PREFIX}/share/defaults/kata-containers/ 2>/dev/null || echo "Failed to list directory"
          fi
          
          # Cleanup
          kubectl delete pod ${POD_NAME} -n default --ignore-not-found=true >/dev/null 2>&1 || true
          
          echo ""
          echo "=== kata-deploy logs ==="
          kubectl logs -n coco-system -l ${{ steps.deployment-params.outputs.daemonset-label }} --tail=200 --prefix=true || echo "No logs available"
          
          if [ "${{ matrix.environment.test-containerd-upgrade }}" == "true" ]; then
            echo ""
            echo "=== containerd-installer logs ==="
            kubectl logs -n coco-system -l app.kubernetes.io/component=containerd-installer --tail=200 --prefix=true || echo "No logs available"
          fi
          
          echo ""
          echo "=== Events ==="
          kubectl get events -A --sort-by='.lastTimestamp' | tail -50

      - name: Uninstall chart
        if: always()
        run: |
          RELEASE_NAME="${{ steps.deployment-params.outputs.release-name }}"
          
          # Check if release name was set (i.e., test actually ran)
          if [ -z "${RELEASE_NAME}" ]; then
            echo "‚è≠Ô∏è  No chart to uninstall (test was skipped)"
            exit 0
          fi
          
          echo "üóëÔ∏è  Uninstalling chart: ${RELEASE_NAME}..."
          helm uninstall ${RELEASE_NAME} -n coco-system --wait --timeout 5m || true
          
          # For containerd upgrade tests, verify rollback
          if [ "${{ matrix.environment.test-containerd-upgrade }}" == "true" ]; then
            echo ""
            echo "‚è≥ Waiting for post-delete cleanup job..."
            
            # Wait for the cleanup job to appear with retry
            TIMEOUT=30
            ELAPSED=0
            CLEANUP_JOB=""
            while [ $ELAPSED -lt $TIMEOUT ]; do
              CLEANUP_JOB=$(kubectl get jobs -n coco-system -l app.kubernetes.io/component=containerd-cleanup -o name 2>/dev/null | head -1 || echo "")
              if [ -n "${CLEANUP_JOB}" ]; then
                echo "‚úÖ Found cleanup job after ${ELAPSED}s: ${CLEANUP_JOB}"
                break
              fi
              sleep 3
              ELAPSED=$((ELAPSED + 3))
            done
            
            if [ -n "${CLEANUP_JOB}" ]; then
              # Wait for the cleanup job to complete
              if kubectl wait --for=condition=complete --timeout=120s "${CLEANUP_JOB}" -n coco-system 2>/dev/null; then
                echo "‚úÖ Cleanup job completed successfully"
                echo "üìã Cleanup job logs:"
                kubectl logs -n coco-system "${CLEANUP_JOB}" --tail=50 || true
              else
                echo "‚ö†Ô∏è  Cleanup job did not complete within timeout"
                kubectl describe "${CLEANUP_JOB}" -n coco-system || true
                kubectl logs -n coco-system "${CLEANUP_JOB}" --tail=100 || true
              fi
            else
              echo "‚ö†Ô∏è  No cleanup job found after ${TIMEOUT}s"
            fi
            
            echo ""
            echo "üîç Checking if containerd was rolled back..."
            
            # Retry getting containerd version after rollback
            TIMEOUT=60
            ELAPSED=0
            AFTER_UNINSTALL_VERSION=""
            while [ $ELAPSED -lt $TIMEOUT ]; do
              AFTER_UNINSTALL_VERSION=$(containerd --version 2>/dev/null | awk '{print $3}' || echo "")
              if [ -n "$AFTER_UNINSTALL_VERSION" ]; then
                echo "‚úÖ Got containerd version after ${ELAPSED}s"
                break
              fi
              sleep 3
              ELAPSED=$((ELAPSED + 3))
            done
            
            if [ -z "$AFTER_UNINSTALL_VERSION" ]; then
              echo "‚ùå Failed to get containerd version after rollback"
              exit 1
            fi
            
            echo "Containerd version after uninstall: ${AFTER_UNINSTALL_VERSION}"
            
            echo ""
            echo "üìä Rollback verification:"
            echo "   Original version:        ${{ steps.original-containerd-version.outputs.version }}"
            echo "   Upgraded version:        ${{ steps.new-containerd-version.outputs.version }}"
            echo "   After uninstall version: ${AFTER_UNINSTALL_VERSION}"
            echo ""
            
            if [ "${AFTER_UNINSTALL_VERSION}" = "${{ steps.original-containerd-version.outputs.version }}" ]; then
              echo "‚úÖ Rollback successful! Containerd restored to original version"
            else
              echo "‚ùå Rollback failed or incomplete!"
              echo "   Expected: ${{ steps.original-containerd-version.outputs.version }}"
              echo "   Got:      ${AFTER_UNINSTALL_VERSION}"
              echo ""
              echo "Checking for cleanup job status:"
              kubectl get jobs -n coco-system -l app.kubernetes.io/component=containerd-cleanup || true
              echo ""
              echo "Checking for cleanup job logs:"
              kubectl logs -n coco-system -l app.kubernetes.io/component=containerd-cleanup --tail=100 || true
              exit 1
            fi
          fi
          
          echo ""
          echo "üîç Verifying cleanup..."
          echo "Remaining pods:"
          kubectl get pods -n coco-system -l app.kubernetes.io/instance=${RELEASE_NAME} || echo "No pods found"
          
          echo ""
          echo "Remaining RuntimeClasses:"
          kubectl get runtimeclass || echo "No RuntimeClasses found"
          
          if [ "${{ matrix.environment.test-containerd-upgrade }}" == "true" ]; then
            echo ""
            echo "Remaining cleanup jobs:"
            kubectl get jobs -n coco-system -l app.kubernetes.io/component=containerd-cleanup || echo "No cleanup jobs found"
          fi

      - name: Test normal pod after cleanup
        if: always() && steps.should-run.outputs.should-run == 'true'
        run: |
          echo "üß™ Testing normal pod creation after cleanup..."
          echo "This ensures containerd still works for regular (non-Kata) workloads"
          
          # Create a simple test pod
          cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: Pod
          metadata:
            name: normal-test-pod
            namespace: default
          spec:
            containers:
            - name: test
              image: quay.io/quay/busybox:latest
              command: ['sh', '-c', 'echo "Hello from normal pod!" && sleep 10']
            restartPolicy: Never
          EOF
          
          # Wait for pod to start with retry
          echo "‚è≥ Waiting for pod to start..."
          TIMEOUT=60
          ELAPSED=0
          while [ $ELAPSED -lt $TIMEOUT ]; do
            POD_PHASE=$(kubectl get pod normal-test-pod -n default -o jsonpath='{.status.phase}' 2>/dev/null || echo "NotFound")
            
            if [ "$POD_PHASE" = "Running" ] || [ "$POD_PHASE" = "Succeeded" ]; then
              echo "‚úÖ Normal pod started successfully after ${ELAPSED}s!"
              kubectl get pod normal-test-pod -n default
              kubectl logs normal-test-pod -n default || true
              break
            elif [ "$POD_PHASE" = "Failed" ]; then
              echo "‚ùå Normal pod failed to start!"
              kubectl describe pod normal-test-pod -n default
              exit 1
            fi
            
            sleep 3
            ELAPSED=$((ELAPSED + 3))
          done
          
          if [ "$POD_PHASE" != "Running" ] && [ "$POD_PHASE" != "Succeeded" ]; then
            echo "‚ùå Timeout: Normal pod did not start"
            kubectl describe pod normal-test-pod -n default
            exit 1
          fi
          
          # Cleanup
          kubectl delete pod normal-test-pod -n default --ignore-not-found=true
          
          echo "‚úÖ Cleanup verification complete - normal pods work correctly!"


  test-summary:
    name: E2E Test Summary
    runs-on: ubuntu-24.04
    needs: [e2e-tests]
    if: always()
    steps:
      - name: Generate summary
        run: |
          echo "# E2E Test Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          
          if [ "${{ needs.e2e-tests.result }}" = "success" ]; then
            echo "‚úÖ E2E Tests (CI + Standard + Containerd Upgrade): **PASSED**" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ needs.e2e-tests.result }}" = "skipped" ]; then
            echo "‚è≠Ô∏è E2E Tests: **SKIPPED** (no changes triggering tests)" >> $GITHUB_STEP_SUMMARY
          else
            echo "‚ùå E2E Tests: **FAILED**" >> $GITHUB_STEP_SUMMARY
          fi
          
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "## Test Coverage" >> $GITHUB_STEP_SUMMARY
          echo "### K8s Distributions" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ k3s (nydus-snapshotter & experimental_force_guest_pull)" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ k0s (nydus-snapshotter & experimental_force_guest_pull)" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ rke2 (nydus-snapshotter & experimental_force_guest_pull)" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ microk8s (nydus-snapshotter & experimental_force_guest_pull)" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ kubeadm with containerd 1.7 (nydus-snapshotter & experimental_force_guest_pull)" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ kubeadm with latest containerd (nydus-snapshotter & experimental_force_guest_pull)" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Features Tested" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ Helm chart installation" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ kata-deploy daemonset deployment" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ RuntimeClass creation" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ Pod scheduling with Kata runtime" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ Image pulling with containerd (nydus snapshotter)" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ Image pulling with containerd (guest-pull mode)" >> $GITHUB_STEP_SUMMARY
          #echo "- ‚úÖ Image pulling with CRI-O" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ CI variant (kata-containers-latest)" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ Standard deployment (CoCo releases)" >> $GITHUB_STEP_SUMMARY
          echo "- ‚úÖ Containerd upgrade and rollback" >> $GITHUB_STEP_SUMMARY

      - name: Check overall tests status
        run: |
          # Check for failures (but allow skipped tests)
          RESULT="${{ needs.e2e-tests.result }}"
          echo "E2E tests result: ${RESULT}"

          if [ "${RESULT}" != "success" ] && [ "${RESULT}" != "skipped" ]; then
            echo "‚ùå E2E tests failed or were cancelled (result: ${RESULT})"
            exit 1
          fi
          echo "‚úÖ All E2E tests passed or were skipped as expected"

  create-issue-on-failure:
    name: Create Issue on E2E Failure
    runs-on: ubuntu-24.04
    needs: [e2e-tests]
    if: |
      always() &&
      github.event_name == 'schedule' &&
      (needs.e2e-tests.result == 'failure' || needs.e2e-tests.result == 'cancelled')
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Check for existing issue
        id: check-issue
        run: |
          echo "üîç Checking for existing open issues..."
          
          # Search for open issues with the failure label
          EXISTING_ISSUES=$(gh issue list \
            --repo "${{ github.repository }}" \
            --label "nightly-e2e-failure" \
            --state open \
            --json number,title \
            --jq '.[] | "\(.number):\(.title)"' || echo "")
          
          if [ -n "${EXISTING_ISSUES}" ]; then
            echo "Found existing open issue(s):"
            echo "${EXISTING_ISSUES}"
            echo "existing=true" >> $GITHUB_OUTPUT
            echo "issues=${EXISTING_ISSUES}" >> $GITHUB_OUTPUT
          else
            echo "No existing open issues found"
            echo "existing=false" >> $GITHUB_OUTPUT
          fi

      - name: Create or update issue
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          WORKFLOW_RUN_URL: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
          WORKFLOW_RUN_NUMBER: ${{ github.run_number }}
          TEST_RESULT: ${{ needs.e2e-tests.result }}
        run: |
          if [ "${{ steps.check-issue.outputs.existing }}" = "true" ]; then
            echo "üìù Updating existing issue..."
            
            # Get the first issue number
            FIRST_ISSUE=$(echo "${{ steps.check-issue.outputs.issues }}" | head -n1 | cut -d':' -f1)
            
            # Add a comment to the existing issue
            gh issue comment "${FIRST_ISSUE}" \
              --repo "${{ github.repository }}" \
              --body "## üîÑ Nightly E2E Tests Failed Again
          
          Another nightly E2E test run has failed.
          
          - **Workflow Run**: [\#${{ github.run_number }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})
          - **Test Result**: \`${{ needs.e2e-tests.result }}\`
          - **Date**: $(date -u +"%Y-%m-%d %H:%M:%S UTC")
          
          Please investigate the failures in the workflow run linked above."
            
            echo "‚úÖ Updated issue #${FIRST_ISSUE}"
          else
            echo "üìù Creating new issue..."
            
            # Create a new issue
            gh issue create \
              --repo "${{ github.repository }}" \
              --title "üö® Nightly E2E Tests Failed - $(date -u +'%Y-%m-%d')" \
              --label "nightly-e2e-failure" \
              --label "bug" \
              --body "## üö® Nightly E2E Tests Failed
          
          The nightly E2E test run has failed. This issue was automatically created to track the failure.
          
          ### Details
          - **Workflow Run**: [\#${{ github.run_number }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})
          - **Test Result**: \`${{ needs.e2e-tests.result }}\`
          - **Date**: $(date -u +"%Y-%m-%d %H:%M:%S UTC")
          - **Branch**: \`${{ github.ref_name }}\`
          - **Commit**: \`${{ github.sha }}\`
          
          ### Next Steps
          1. Review the [workflow run](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}) to identify the failing tests
          2. Check the test logs for error messages
          3. Investigate if this is a regression or an environmental issue
          4. Fix the issue and verify tests pass
          5. Close this issue once resolved
          
          ### Test Coverage
          The E2E tests cover:
          - Multiple Kubernetes distributions (k3s, k0s, rke2, microk8s, kubeadm)
          - Different image pull modes (nydus-snapshotter, experimental-force-guest-pull)
          - CI and standard deployment variants
          - Containerd upgrade scenarios
          
          ---
          
          *This issue was automatically created by the nightly E2E test workflow.*"
            
            echo "‚úÖ Created new issue"
          fi

